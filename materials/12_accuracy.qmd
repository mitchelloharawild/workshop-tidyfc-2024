---
title: "Time Series Analysis & Forecasting Using R"
subtitle: "12. Accuracy evaluation"
format:
  beamer:
    pdf-engine: pdflatex
    aspectratio: 169
    fontsize: "14pt,t"
    section-titles: false
    knitr:
      opts_chunk:
        dev: "cairo_pdf"
    fig-width: 7.5
    fig-height: 3.5
    include-in-header: header.tex
    keep-tex: false
---

## Outline

\vspace*{0.7cm}\tableofcontents


```{r}
#| label: setup
#| include: false
#| cache: false
source("setup.R")
h02 <- tsibbledata::PBS |>
  filter(ATC2 == "H02") |>
  summarise(Cost = sum(Cost))
melsyd <- tsibbledata::ansett |>
  filter(Airports == "MEL-SYD")
```

# Forecasting recap

## Tidy time series data

Use `as_tsibble()` to convert a dataset into a tsibble.

Identify which column(s) are:

* The `index` variable
* Identifying `key` variable(s)
* Measured variable(s)

## Visualising time series

* Time plot: `data |> autoplot(y)`
* Season plot: `data |> gg_season(y)`
* Seasonal subseries plot: `data |> gg_subseries(y)`
* Lag plot: `data |> gg_lag(y)`
* ACF plot: `data |> ACF(y) |> autoplot()`

## Transformations and decompositions

Simplify patterns with transformations:

* Population and inflation adjustments
* Mathematical transformations (`log()`, `sqrt()`, `box_cox()`)

Separate trend and seasonal patterns with decomposition:

* `STL()` decomposition (additive, choose windows)
* Extract decomposition with `components()`
* Produce seasonally adjusted data for decision making

## Forecasting basics

Estimate a model on data with `model()`

Benchmark forecasting methods:

* Simple average: `MEAN(y)`
* Naive method: `NAIVE(y)`
* Seasonal naive method: `SNAIVE(y)`
* RW w/ drift: `RW(y ~ drift())`

# Residual diagnostics

## Fitted values

 - $\hat{y}_{t|t-1}$ is the forecast of $y_t$ based on observations $y_1,\dots,y_t$.
 - We call these "fitted values".
 - Sometimes drop the subscript: $\hat{y}_t \equiv \hat{y}_{t|t-1}$.
 - Often not true forecasts since parameters are estimated on all data.

### For example:

 - $\hat{y}_{t} = \bar{y}$ for average method.
 - $\hat{y}_{t} = y_{t-1} + (y_{T}-y_1)/(T-1)$ for drift method.

## Forecasting residuals

\begin{block}{}
\textbf{Residuals in forecasting:} difference between observed value and its fitted value: $e_t = y_t-\hat{y}_{t|t-1}$.
\end{block}
\pause\fontsize{13}{15}\sf

\alert{Assumptions}

  1. $\{e_t\}$ uncorrelated. If they aren't, then information left in  residuals that should be used in computing forecasts.
  2. $\{e_t\}$ have mean zero. If they don't, then forecasts are biased.

\pause

\alert{Useful properties} (for prediction intervals)

  3. $\{e_t\}$ have constant variance.
  4. $\{e_t\}$ are normally distributed.

## Total in-school staff
\fontsize{9}{10}\sf

```{r fbf}
total_staff <- staff |>
  summarise(Count = sum(`In-School Staff Count`))
total_staff |> autoplot(Count)
```

## Total in-school staff
\fontsize{10}{10}\sf

```{r augment}
fit <- total_staff |> model(RW(Count ~ drift()))
augment(fit)
```

## Total in-school staff
\fontsize{10}{10}\sf

```{r dj4, echo=TRUE, warning=FALSE, fig.height=3.4, dependson="augment"}
augment(fit) |>
  ggplot(aes(x = Year)) +
  geom_line(aes(y = Count, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted"))
```

## Total in-school staff
\fontsize{10}{10}\sf

```{r dj5, echo=TRUE, warning = FALSE, dependson="augment"}
augment(fit) |>
  autoplot(.resid) +
  labs(x = "Day", y = "", title = "Residuals from RW w/ drift method")
```

## Total in-school staff
\fontsize{11}{11}\sf

```{r dj6, warning=FALSE, fig.height=3.4, dependson="augment"}
augment(fit) |>
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 10) +
  labs(title = "Histogram of residuals")
```

## Total in-school staff
\fontsize{11}{11}\sf

```{r dj7, dependson="augment"}
augment(fit) |>
  ACF(.resid) |>
  autoplot() + labs(title = "ACF of residuals")
```

## ACF of residuals

  * We assume that the residuals are white noise (uncorrelated, mean zero, constant variance). If they aren't, then there is information left in  the residuals that should be used in computing forecasts.

  * So a standard residual diagnostic is to check the ACF of the residuals of a forecasting method.

  * We *expect* these to look like white noise.

## Combined diagnostic graph
\fontsize{11}{11}\sf

```{r dj8, dependson="augment"}
fit |> gg_tsresiduals()
```

## Ljung-Box test
\fontsize{12}{13}\sf

Test whether *whole set* of $r_{k}$ values is significantly different from zero set.

\begin{block}{}
\centerline{$\displaystyle
 Q = T(T+2) \sum_{k=1}^\ell (T-k)^{-1}r_k^2$\qquad
where $\ell=$ max lag and $T=$ \# observations}
\end{block}

  * If each $r_k$ close to zero, $Q$ will be **small**.
  * If some $r_k$ values large ($+$ or $-$), $Q$ will be **large**.
  * My preferences: $h=10$ for non-seasonal data, $h=2m$ for seasonal data.
  * If data are WN and $T$ large, $Q \sim \chi^2$ with $\ell$ degrees of freedom.

## Ljung-Box test
\fontsize{12}{13}\sf

\begin{block}{}
\centerline{$\displaystyle
 Q = T(T+2) \sum_{k=1}^\ell (T-k)^{-1}r_k^2$\qquad
where $\ell=$ max lag and $T=$ \# observations.}
\end{block}

\fontsize{11}{11}\sf

```{r dj9extra, echo=FALSE, fig.height=1.65}
augment(fit) |>
  ACF(.resid, lag_max = 10) |>
  autoplot() + labs(title = "ACF of residuals")
```

\vspace*{-0.3cm}

```{r dj9, echo=TRUE, dependson="augment"}
# lag = h
augment(fit) |> features(.resid, ljung_box, lag = 10)
```

# Lab Session 10
## Lab Session 10

  * Compute RW w/ drift forecasts for total student enrolments in Australia (`students`).

  * Test if the residuals are white noise. What do you conclude?

# Forecast accuracy measures

## Training and test sets

```{r traintest, fig.height=1, echo=FALSE}
train <- 1:18
test <- 19:24
par(mar = c(0, 0, 0, 0))
plot(0, 0, xlim = c(0, 26), ylim = c(0, 2), xaxt = "n", yaxt = "n", bty = "n", xlab = "", ylab = "", type = "n")
arrows(0, 0.5, 25, 0.5, 0.05)
points(train, train * 0 + 0.5, pch = 19, col = "blue")
points(test, test * 0 + 0.5, pch = 19, col = "red")
text(26, 0.5, "time")
text(10, 1, "Training data", col = "blue")
text(21, 1, "Test data", col = "red")
```

  * A model which fits the training data well will not necessarily forecast well.
  * Forecast accuracy is based only on the test set.

### Forecast errors

Forecast "error": the difference between an observed value and its forecast.
$$
  e_{T+h} = y_{T+h} - \hat{y}_{T+h|T},
$$
where the training data is given by $\{y_1,\dots,y_T\}$

## Measures of forecast accuracy
\fontsize{11}{12}\sf

```r
beer_fit <- aus_production |>
  filter(between(year(Quarter), 1992, 2007)) |>
  model(
    snaive = SNAIVE(Beer),
    mean = MEAN(Beer)
  )
beer_fit |>
  forecast(h = "3 years") |>
  autoplot(aus_production, level = NULL) +
  labs(title ="Forecasts for quarterly beer production",
       x ="Year", y ="Megalitres") +
  guides(colour = guide_legend(title = "Forecast"))
```

## Measures of forecast accuracy

```{r beer-fc-1, echo=FALSE, fig.height=4}
beer_fit <- aus_production |>
  filter(between(year(Quarter), 1992, 2007)) |>
  model(
    snaive = SNAIVE(Beer),
    mean = MEAN(Beer)
  )
beer_fit |>
  forecast(h = "3 years") |>
  autoplot(aus_production, level = NULL) +
  labs(
    title = "Forecasts for quarterly beer production",
    x = "Year", y = "Megalitres"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
```

## Measures of forecast accuracy

\begin{tabular}{rl}
$y_{T+h}=$ & $(T+h)$th observation, $h=1,\dots,H$ \\
$\pred{y}{T+h}{T}=$ & its forecast based on data up to time $T$. \\
$e_{T+h} =$  & $y_{T+h} - \pred{y}{T+h}{T}$
\end{tabular}

\begin{block}{}\vspace*{-0.7cm}
\begin{align*}
\text{ME} &= \text{mean}(e_{T+h})
\end{align*}\vspace*{-0.9cm}
\end{block}

* Mean error is an indicator of bias.
* On training accuracy, it is expected to be 0.


## Measures of forecast accuracy

\begin{tabular}{rl}
$y_{T+h}=$ & $(T+h)$th observation, $h=1,\dots,H$ \\
$\pred{y}{T+h}{T}=$ & its forecast based on data up to time $T$. \\
$e_{T+h} =$  & $y_{T+h} - \pred{y}{T+h}{T}$
\end{tabular}

\begin{block}{}\vspace*{-0.7cm}
\begin{align*}
\text{MAE} &= \text{mean}(|e_{T+h}|) \\[-0.2cm]
\text{MSE} &= \text{mean}(e_{T+h}^2) \qquad
&&\text{RMSE} &= \sqrt{\text{mean}(e_{T+h}^2)} \\[-0.1cm]
\text{MAPE} &= 100\text{mean}(|e_{T+h}|/ |y_{T+h}|)
\end{align*}\vspace*{-0.9cm}
\end{block}
\pause

  * MAE, MSE, RMSE are all scale dependent.
  * MAPE is scale independent but is only sensible if $y_t\gg 0$ for all $t$, and $y$ has a natural zero.

## Measures of forecast accuracy
\fontsize{13}{15}\sf

\begin{block}{Mean Absolute Scaled Error}
$$
  \text{MASE} = \text{mean}(|e_{T+h}|/Q)
$$
\end{block} \pause

- For non-seasonal series, scale uses na誰ve forecasts:

\centerline{$Q = \frac{1}{T-1}\displaystyle\sum_{t=2}^T |y_{t}-y_{t-1}|$}

- For seasonal series, scale uses seasonal na誰ve forecasts:

\centerline{$Q = \frac{1}{T-m}\displaystyle\sum_{t=m+1}^T |y_{t}-y_{t-m}|$}
\rightline{where $m$ is the seasonal frequency}\pause

Proposed by Hyndman and Koehler (IJF, 2006).

## Measures of forecast accuracy
\fontsize{13}{15}\sf

\begin{block}{Root Mean Squared Scaled Error}
$$
  \text{RMSSE} = \sqrt{\text{mean}(e^2_{T+h}/Q)}
$$
\end{block}

- For non-seasonal series, scale uses na誰ve forecasts:

\centerline{$Q = \frac{1}{T-1}\displaystyle\sum_{t=2}^T (y_{t}-y_{t-1})^2$}

- For seasonal series, scale uses seasonal na誰ve forecasts:

\centerline{$Q = \frac{1}{T-m}\displaystyle\sum_{t=m+1}^T (y_{t}-y_{t-m})^2$}
\rightline{where $m$ is the seasonal frequencyq}

Proposed by Hyndman and Koehler (IJF, 2006).

## Measures of forecast accuracy

\fontsize{9.8}{10}\sf

```{r beer-test-accuracy, dependson='beer-fc-1'}
beer_fc <- forecast(beer_fit, h = "3 years")
accuracy(beer_fc, aus_production)
```

# Lab Session 11
## Lab Session 11

 * Create a training set for employed Australian students (`student_labour`) by withholding the last four years as a test set.
 * Fit all the appropriate benchmark methods to the training set and forecast the periods covered by the test set.
 * Compute the accuracy of your forecasts. Which method does best?
 
::: {.callout-note}
## Finished early?

Repeat the exercise using the Australian takeaway food turnover data (`aus_retail`) with a test set of four years.
:::